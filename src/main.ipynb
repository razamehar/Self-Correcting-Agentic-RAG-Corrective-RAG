{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:27.946098Z",
     "start_time": "2025-06-22T01:50:27.919581Z"
    }
   },
   "source": [
    "from pprint import pprint\n",
    "from config.api_keys import APIKeysConfig\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "from utils.retrievers import (\n",
    "    build_keyword_retriever,\n",
    "    build_parent_document_retriever,\n",
    "    create_splitters,\n",
    "    initialize_vectorstore,\n",
    "    build_ensemble_retriever\n",
    ")\n",
    "\n",
    "from utils.data_models import GraphState\n",
    "from utils.utils import load_and_preprocess_documents\n",
    "from utils.chain import (\n",
    "    get_generate_queries_chain,\n",
    "    get_rag_chain,\n",
    "    get_retrieval_grader,\n",
    "    get_question_rewriter,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:36.003451Z",
     "start_time": "2025-06-22T01:50:28.432050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "docs = load_and_preprocess_documents('../data/pdf.pdf')\n",
    "parent_splitter, child_splitter = create_splitters()\n",
    "\n",
    "vectorstore = initialize_vectorstore(docs, embeddings)\n",
    "parent_retriever = build_parent_document_retriever(vectorstore, parent_splitter, child_splitter)\n",
    "keyword_retriever = build_keyword_retriever(docs)\n",
    "ensemble_retriever = build_ensemble_retriever(parent_retriever, keyword_retriever)\n",
    "generate_queries = get_generate_queries_chain(llm)\n",
    "rag_chain = get_rag_chain(llm)\n",
    "\n",
    "retrieval_grader = get_retrieval_grader(llm)\n",
    "question_rewriter = get_question_rewriter(llm)\n",
    "web_search_tool = TavilySearch(max_results=3)"
   ],
   "id": "8f3316847dc7a6d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\My Projects\\Self-Correcting Agentic RAG Corrective RAG\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1772: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:36.049788Z",
     "start_time": "2025-06-22T01:50:36.036747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve documents using the question in state\n",
    "def retrieve(state):\n",
    "    print(\"RETRIEVE: Retrieving documents from vectorstore...\")\n",
    "    question = state[\"question\"]\n",
    "    retrieved_documents = ensemble_retriever.invoke(question)\n",
    "    return {\"documents\": retrieved_documents, \"question\": question}\n",
    "\n",
    "# Generate an answer using the retrieved documents and question\n",
    "def generate(state):\n",
    "    print(\"GENERATE: Generating response using retrieved documents and question...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "# Filter documents based on relevance to the question\n",
    "def grade_documents(state):\n",
    "    print(\"GRADE: Checking document relevance to question...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        relevance = score.relevance_score\n",
    "        print(f\"The relevance score for this document is: {relevance}\")\n",
    "        if relevance >= 50:\n",
    "            print(\"\\tGRADE: Document is relevant\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"\\tGRADE: Document is NOT relevant\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "# Rewrite the question to improve retrieval quality\n",
    "def transform_query(state):\n",
    "    print(\"TRANSFORM QUERY: Rewriting the query...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "# Perform web search and append results to existing documents\n",
    "def web_search(state):\n",
    "    print(\"WEB SEARCH: Performing web search...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    print(web_docs)\n",
    "    results = web_docs.get(\"results\", [])\n",
    "    if not results:\n",
    "        print(\"No web search results found.\")\n",
    "        return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "    web_results = \"\\n\".join([res.get(\"content\", \"\") for res in results if \"content\" in res])\n",
    "    if web_results:\n",
    "        documents.append(Document(page_content=web_results))\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# Decide whether to generate an answer or transform the query\n",
    "def decide_to_generate(state):\n",
    "    print(\"DECISION: Assessing graded documents...\")\n",
    "    web_search_flag = state[\"web_search\"]\n",
    "\n",
    "    if web_search_flag == \"Yes\":\n",
    "        print(\"\\tDECISION: All documents NOT relevant, transforming query\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"\\tDECISION: Documents relevant, generating response\")\n",
    "        return \"generate\""
   ],
   "id": "189351ceb1d462e0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:36.065233Z",
     "start_time": "2025-06-22T01:50:36.054847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()"
   ],
   "id": "635d15ac08623b33",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:36.111025Z",
     "start_time": "2025-06-22T01:50:36.096033Z"
    }
   },
   "cell_type": "code",
   "source": "inputs = {\"question\": \"Pretraining data toxicity\"}",
   "id": "2e812aaa7720e9b2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:43.017072Z",
     "start_time": "2025-06-22T01:50:38.341476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Node '{key}':\")"
   ],
   "id": "c6ce84cc740b8b47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVE: Retrieving documents from vectorstore...\n",
      "\"Node 'retrieve':\"\n",
      "GRADE: Checking document relevance to question...\n",
      "The relevance score for this document is: 90.0\n",
      "\tGRADE: Document is relevant\n",
      "The relevance score for this document is: 95.0\n",
      "\tGRADE: Document is relevant\n",
      "The relevance score for this document is: 82.0\n",
      "\tGRADE: Document is relevant\n",
      "The relevance score for this document is: 60.0\n",
      "\tGRADE: Document is relevant\n",
      "The relevance score for this document is: 100.0\n",
      "\tGRADE: Document is relevant\n",
      "DECISION: Assessing graded documents...\n",
      "\tDECISION: Documents relevant, generating response\n",
      "\"Node 'grade_documents':\"\n",
      "GENERATE: Generating response using retrieved documents and question...\n",
      "\"Node 'generate':\"\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T01:50:45.015387Z",
     "start_time": "2025-06-22T01:50:45.005006Z"
    }
   },
   "cell_type": "code",
   "source": "pprint(value[\"generation\"])",
   "id": "833059378afaabe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pretraining data toxicity is measured to have a small amount of toxicity, '\n",
      " 'with about 0.2% of documents rated 0.5 or higher on a toxicity likelihood '\n",
      " 'scale. The models that are less aggressively filtered in their pretraining '\n",
      " 'data show lower toxicity levels. The choice not to scrub toxic data from '\n",
      " 'pretraining aims to allow for better downstream generalization, but '\n",
      " 'additional safety mitigation measures should be considered before deploying '\n",
      " 'these models.')\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
